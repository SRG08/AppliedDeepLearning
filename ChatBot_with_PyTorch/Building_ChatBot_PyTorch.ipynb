{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building_Chat_bot_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AEFXZWBbUN-M",
        "XKsbUvfAowB1",
        "s63YNvY2o6NK",
        "y7gGFioXQx2-",
        "IVpV1Q9XXyUX",
        "IeXQI_LXT82b",
        "_wPECakmbQcc",
        "JZ-0Nttssud2",
        "Jk4wuKE8tIaO",
        "IVBP_m1ui8LA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEFXZWBbUN-M"
      },
      "source": [
        "# Basic Imports and Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dyorsFn9vB9"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn                # This is for importing Neural networks\n",
        "from torch import optim              # Importing Optimisers\n",
        "import torch.nn.functional as F      # Activation functions ,such as Relu, Softmax etc\n",
        "import csv                           # Importing CSV reader and writer\n",
        "import random                        # This is to create random numbers\n",
        "import re                            # Support for regular expressions (RE)\n",
        "import unicodedata                   # This module provides access to the Unicode Character Database which defines character properties for all Unicode characters.\n",
        "import itertools                     # Functional tools for creating and using iterators. Infinite iterators:\n",
        "import codecs                        # codecs -- Python Codec Registry\n",
        "import numpy                         # for numpy and in-place operation\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt      # For visualisation\n",
        "from google.colab import files       # Importing files to google colab\n",
        "import pandas as pd                  # Import txt files\n",
        "import tqdm                          # progress bar for checking progress\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eefBMRCL91eX",
        "outputId": "b6cb1fb2-a170-42c6-fda1-05ce456ed0e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Changing from cuda to cpu and cpu to cuda\n",
        "\n",
        "isCUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if isCUDA else \"cpu\")\n",
        "print(isCUDA , \",\" , device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True , cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSquUN_DJ4Re"
      },
      "source": [
        "# Preprocessing Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKsbUvfAowB1"
      },
      "source": [
        "## Data Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7a7y_zWfTwY"
      },
      "source": [
        "Snippet of the corpus:\n",
        "\n",
        "**raw_script_urls.txt** :\n",
        "m450 +++$+++ my girl 2 +++$+++ http://www.scifiscripts.com/msol/my_girl_2.html\n",
        "<br>\n",
        "\n",
        "**movie_characters_metadata.txt** :\n",
        "u6745 +++$+++ BOY +++$+++ m450 +++$+++ my girl 2 +++$+++ ? +++$+++ ? <br>\n",
        "\n",
        "**movie_titles_metadata** \n",
        "<br>\n",
        "m450 +++$+++ my girl 2 +++$+++ 1994 +++$+++ 4.80 +++$+++ 5689 +++$+++ ['comedy', 'drama', 'family', 'romance']\n",
        "\n",
        "**movie_conversations.txt**\n",
        "<br>\n",
        "u6745 +++$+++ u6760 +++$+++ m450 +++$+++ ['L403280', 'L403281', 'L403282', 'L403283', 'L403284', 'L403285', 'L403286', 'L403287', 'L403288']\n",
        "\n",
        "u6745 +++$+++ u6760 +++$+++ m450 +++$+++ ['L403289', 'L403290', 'L403291', 'L403292']\n",
        "\n",
        "**movie_lines.txt**\n",
        "<br>\n",
        "L403280 +++$+++ u6760 +++$+++ m450 +++$+++ VADA +++$+++ How did you know my name?<br>\n",
        "L403281 +++$+++ u6745 +++$+++ m450 +++$+++ BOY +++$+++ Your Uncle Phil told me. <br>\n",
        "L403282 +++$+++ u6760 +++$+++ m450 +++$+++ VADA +++$+++ Where is he?? He was supposed to meet me.<br>\n",
        "L403283 +++$+++ u6745 +++$+++ m450 +++$+++ BOY +++$+++ Hey relax, you think I kidnapped him or something?<br>\n",
        "L403284 +++$+++ u6760 +++$+++ m450 +++$+++ VADA +++$+++ This is California, anything is possible.<br>\n",
        "L403285 +++$+++ u6745 +++$+++ m450 +++$+++ BOY +++$+++ Well if I was looking for a victim, I definitely wouldn't pick your Uncle Phil who outweighs me by about 150 pounds, besides, who would I ask for ransom? You??<br>\n",
        "L403286 +++$+++ u6760 +++$+++ m450 +++$+++ VADA +++$+++ Are you suffering from a chemical imbalance or is it just an attitude problem.<br>\n",
        "L403287 +++$+++ u6745 +++$+++ m450 +++$+++ BOY +++$+++ My only problem is that your Uncle Phil is giving me five bucks to pick you up but I don't get paid 'till delivery.<br>\n",
        "L403288 +++$+++ u6760 +++$+++ m450 +++$+++ VADA +++$+++ Gee, that is a problem.<br>\n",
        "L403289 +++$+++ u6760 +++$+++ m450 +++$+++ VADA +++$+++ Put that down, I'll...I'll call the police!<br>\n",
        "L403290 +++$+++ u6745 +++$+++ m450 +++$+++ BOY +++$+++ What are you gonna do? Tell them that...a polite person helped carry your bag?<br>\n",
        "L403291 +++$+++ u6760 +++$+++ m450 +++$+++ VADA +++$+++ I don't think you're very polite.<br>\n",
        "L403292 +++$+++ u6745 +++$+++ m450 +++$+++ BOY +++$+++ Yeah, well I don't think you're very grateful. A lot of people in your position would say &quot;thank you&quot;.<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s63YNvY2o6NK"
      },
      "source": [
        "## Importing Data and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQCV_mkBrj4l"
      },
      "source": [
        "# Code for mounting operations to Google Colab\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/MyDrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN_DURa3e0J0",
        "outputId": "81876c2d-e665-4b6e-c21b-1ac90b2c2ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Conversation_data = '/content/drive/My Drive/Datasets_Data_Science/cornell movie-dialogs corpus/movie_conversations.txt'\n",
        "with open(Conversation_data , 'r' ) as con_file:\n",
        "  Con_lines = con_file.readlines()\n",
        "  # print(Con_lines[:10])                           \n",
        "  for Con_line in Con_lines[:10]:\n",
        "    print(Con_line.strip())                # reading lines and stripping spaces and line indentation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuDn8mva-CA9",
        "outputId": "4428a03e-4c9a-44a3-b861-bdcb02b93734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Lines_data = \"/content/drive/My Drive/Datasets_Data_Science/cornell movie-dialogs corpus/movie_lines.txt\"\n",
        "\n",
        "with open(Lines_data , encoding=\"ISO-8859-1\" , mode='r') as Li_file:   # Challenges_1 : If you get \"'utf-8' codec can't decode byte\" error use encoding as \"ISO-8859-1\" encoding.\n",
        "  Lines_lines = Li_file.readlines()\n",
        "\n",
        "  for Li_lines in Lines_lines[:10]:\n",
        "    print(Li_lines.strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
            "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
            "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
            "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
            "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
            "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
            "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
            "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n",
            "L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
            "L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twFYOyA2LtKn"
      },
      "source": [
        "# with open(Lines_data , encoding=\"ISO-8859-1\" , mode='r') as f:\n",
        "#   for Lines_line in f:\n",
        "#     values = Lines_line.split(\" +++$+++ \")\n",
        "#     print(values[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS5tUydAOxuG"
      },
      "source": [
        "# line_fields = [\"lineID\" , \"characterID\" , \"movieID\", \"character\" , \"text\"]\n",
        "# values = [\"L1045\", \"u0\", \"m0\", \"BIANCA\" ,  \"They do to!\"]\n",
        "\n",
        "# obj= {}\n",
        "# for i, field in enumerate(line_fields):\n",
        "#   obj[field] = values[i]\n",
        "\n",
        "# print(obj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyCCdy3jJHFW"
      },
      "source": [
        "# Split the line of the file into a dictionary of fields (lineID , characterID , movieID , character , text) as we do not want \"+++$+++\" for \"movie_lines.txt\"\n",
        "\n",
        "line_fields = [\"lineID\" , \"characterID\" , \"movieID\", \"character\" , \"text\"]\n",
        "\n",
        "Li_lines = {}\n",
        "\n",
        "with open(Lines_data , encoding=\"ISO-8859-1\" , mode='r') as f:\n",
        "  for Lines_line in f:\n",
        "    values = Lines_line.split(\" +++$+++ \")\n",
        "\n",
        "    # Extract Fields\n",
        "    lineobj = {}                           # define a dictionary\n",
        "\n",
        "    for i , field in enumerate(line_fields):\n",
        "      lineobj[field] = values[i]\n",
        "\n",
        "      \n",
        "    Li_lines[lineobj[\"lineID\"]] = lineobj            # Key is the line ID and value is all the sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYdbOKmsDD7p"
      },
      "source": [
        "# dict(list(Li_lines.items())[0:2])  # printing just 2 values to check , as we get \"NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\" error in Google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D3NRGEvojw-"
      },
      "source": [
        "# Li_lines[\"L194\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS2xQiFRHmKB"
      },
      "source": [
        "# Split the line of the file into a dictionary of fields (Character_1ID , Character_2ID , movieID , ConversationID) as we do not want \"+++$+++\" for \"movie_conversations.txt\"\n",
        "\n",
        "Conv_lables = [\"Character_1ID\",\"Character_2ID\" , \"movieID\" , \"ConversationID\" ]\n",
        "\n",
        "Converse_list = []\n",
        "\n",
        "with open(Conversation_data , encoding=\"ISO-8859-1\" , mode=\"r\") as conversations:\n",
        "  for conv  in conversations:\n",
        "    Converse = conv.split(\" +++$+++ \")\n",
        "\n",
        "    # Extracting values for lables\n",
        "    Con_obj={}\n",
        "    for i, item in enumerate(Conv_lables):\n",
        "      Con_obj[item]=Converse[i]\n",
        "  \n",
        "    # Convert string results from split to list , since con_obj[\"ConversationID\"] = \"['L666520', 'L666521', 'L666522']\\n\"\n",
        "\n",
        "    lineIds = eval(Con_obj[\"ConversationID\"])        # Check the appendix in the last cell of this notebook\n",
        "    # output: ['L666520', 'L666521', 'L666522']  \n",
        "\n",
        "    # reassemble lines\n",
        "    Con_obj[\"lines\"]  = []                          # we are adding all the values to the newly created key called \"lines\"\n",
        "    for lineId in lineIds:\n",
        "      Con_obj[\"lines\"].append(Li_lines[lineId])\n",
        "\n",
        "    Converse_list.append(Con_obj)       # Here we are just appending the structure to list \n",
        "\n",
        "# Converse_list[0:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-pZBOEuvGcI",
        "outputId": "5fdef0a0-4b58-44b0-fe36-65311be21208",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"length of the lines\" , len(Converse_list[0][\"lines\"])) # length of every element in lines block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of the lines 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDSVbXTOvmdJ",
        "outputId": "b664a7c4-d609-44be-b2ef-bf4b089a6dc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Converse_list[0][\"lines\"][0][\"text\"].strip())  # Our goal is to extract all the text from the lists / elements in the lines block\n",
        "print(Converse_list[0][\"lines\"][1][\"text\"].strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_OB-EJCoKpT"
      },
      "source": [
        "# Extract Pairs of Sentence from Conversation\n",
        "\n",
        "qa_pair = []\n",
        "\n",
        "for conversation in Converse_list:\n",
        "\n",
        "  for i in range(len(conversation[\"lines\"])-1):\n",
        "    question = conversation[\"lines\"][i][\"text\"].strip()     # this is question which we are extracting from the block of first conversation ID\n",
        "    answer = conversation[\"lines\"][i+1][\"text\"].strip()     # this is answer corrusponding to the question which we are extracting from the block of first conversation ID\n",
        "\n",
        "    # Filter Condition \"igonore if question and answer is empty\"\n",
        "\n",
        "    if question and answer:\n",
        "      qa_pair.append([question , answer])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c51SO3aVoK2-",
        "outputId": "f68de131-4be8-4d04-f7be-a545e7b34f89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "qa_pair[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
              "  \"Well, I thought we'd start with pronunciation, if that's okay with you.\"],\n",
              " [\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
              "  'Not the hacking and gagging and spitting part.  Please.'],\n",
              " ['Not the hacking and gagging and spitting part.  Please.',\n",
              "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"],\n",
              " [\"You're asking me out.  That's so cute. What's your name again?\",\n",
              "  'Forget it.'],\n",
              " [\"No, no, it's my fault -- we didn't have a proper introduction ---\",\n",
              "  'Cameron.'],\n",
              " ['Cameron.',\n",
              "  \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\"],\n",
              " [\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
              "  'Seems like she could get a date easy enough...'],\n",
              " ['Why?',\n",
              "  'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.'],\n",
              " ['Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
              "  \"That's a shame.\"],\n",
              " ['Gosh, if only we could find Kat a boyfriend...',\n",
              "  'Let me see what I can do.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH2xC073oKx_",
        "outputId": "ef53ea4d-495f-4327-83b4-69d5ea806eb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define path to write qa_pair to new file\n",
        "\n",
        "New_path = '/content/drive/My Drive/Datasets_Data_Science/cornell movie-dialogs corpus/New_formatted_Movie_conversations.txt'  # Saving the file as a new formatted file\n",
        "\n",
        "delimiter = '\\t'\n",
        "\n",
        "# Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# Writing the txt file\n",
        "with open(New_path , encoding=\"utf-8\" , mode=\"w\") as Outfile:\n",
        "\n",
        "  writer = csv.writer(Outfile, delimiter = delimiter)\n",
        "\n",
        "  for pair in tqdm.tqdm(qa_pair):\n",
        "    writer.writerow(pair)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 221282/221282 [00:00<00:00, 268284.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7gGFioXQx2-"
      },
      "source": [
        "## Importing new Formatted File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_7ehF9SoKu9",
        "outputId": "127a7eec-29b5-4c4d-c5c3-0fb045b54c5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Visualising the file if it is properly written\n",
        "\n",
        "New_formatted = '/content/drive/My Drive/Datasets_Data_Science/cornell movie-dialogs corpus/New_formatted_Movie_conversations.txt'\n",
        "\n",
        "with open(New_formatted ,mode=\"rb\") as formatted:\n",
        "  lines = formatted.readlines()\n",
        "\n",
        "  for line in (lines[:8]):\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\n\"\n",
            "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\n\"\n",
            "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\n\"\n",
            "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\n\"\n",
            "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\n\"\n",
            "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\n\"\n",
            "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\n\"\n",
            "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCOabz4SCdlf"
      },
      "source": [
        "# we will be creating word vocablary for the newly formatted file\n",
        "\n",
        "PAD_TOKEN = 0  # Used for padding short sentence  , as at the end we are going to use fixed size length\n",
        "SOS_TOKEN = 1  # Start of sentence token <START>\n",
        "EOS_TOKEN = 2  # End of sentence token <END>\n",
        "\n",
        "class Vocabulary:\n",
        "\n",
        "  def __init__(self , name):\n",
        "    self.name=name               # Name of the dateset we are using\n",
        "    self.trimmed = False         # Setting Trim false\n",
        "    self.word2index = {}         # we are creating dictionary for word to index\n",
        "    self.word2count = {}         # we are creatung dictionary to count number of words to filter the less count of words\n",
        "    \n",
        "    self.index2word = {PAD_TOKEN:\"PAD\" , SOS_TOKEN:\"SOS\" , EOS_TOKEN:\"EOS\"}         # we are giving the custome index for the words so that it can be identifible as per LSTM encoder and decoder\n",
        "    self.num_words = 3           # Count of default TOKENS PAD , SOS , EOS\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  def addSentence(self , sentence):      # we are trying to split the sentence and add the sentence to word2index with other function called addWord()\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "\n",
        "  def addWord(self, word):               # we are adding words to word2index if they are not present in word2index\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.num_words  # We are starting with 3 as index \"0\" is PAD_TOKEN , \"1\" is START_TOKEN , \"2\" is END_TOKEN and then increment num_words below\n",
        "      self.word2count[word] = 1               # count the words in the word2index already exists or not if it exists increment below , based on this we will filter below\n",
        "      self.index2word[self.num_words] = word # Incerement the word index after 3 the word would be 4 hence index2word[4] = word , this is opposite of word2index as we are mapping.\n",
        "      self.num_words +=1                      # Incrementing the num_words count after adding word to word2count\n",
        "\n",
        "    else:\n",
        "      self.word2count[word] +=1               # Incrementing value of word2count if count already found as \"1\"\n",
        "\n",
        "\n",
        "      # Remove words below certain criteria (filtering words based in Count)\n",
        "\n",
        "  def trim(self, min_count):                 # paremeter min_count to imput the criteria for filtering\n",
        "    if self.trimmed:\n",
        "      return\n",
        "    self.trimmed = True\n",
        "\n",
        "    keep_words = []                          # Collecting all the words above minimum thrishold\n",
        "\n",
        "    for k , v in self.word2count.items():        # Taking the key , value in word2count.items()\n",
        "      if v >= min_count:                    # checking if the value is above the thrishold (min_count)\n",
        "        keep_words.append(k)                 # appending key if the value is above than thrishold\n",
        "\n",
        "\n",
        "\n",
        "    print(\"keep_words {} / {} = {:0.4f}\".format(len(keep_words) , len(self.word2index), len(keep_words) / len(self.word2index)))\n",
        "\n",
        "\n",
        "    # Reinitialise the dictionaries as they are updated again after trim ,  and previously they had older words\n",
        "\n",
        "    self.word2index = {}         # we are creating dictionary for word to index\n",
        "    self.word2count = {}         # we are creatung dictionary to count number of words to filter the less count of words\n",
        "    # print(dict(list(self.word2count.items())))\n",
        "  \n",
        "    self.index2word = {PAD_TOKEN:\"PAD\" , SOS_TOKEN:\"SOS\" , EOS_TOKEN:\"EOS\"}         # we are giving the custome index for the words so that it can be identifible as per LSTM encoder and decoder\n",
        "    self.num_words = 3           # Count of default TOKENS PAD , SOS , EOS\n",
        "\n",
        "\n",
        "    for word in keep_words:\n",
        "      self.addWord(word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVpV1Q9XXyUX"
      },
      "source": [
        "## Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yMH5Vg4Cerv"
      },
      "source": [
        "# Truen Unicode string to plain ASCII , to remove the  accents (normalize) from the text Example given below \n",
        "# Consider you are writing a code that compares strings in the various language if you do normal comparison it may give inaccurate results.\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize(\"NFD\" , s) if unicodedata.category(c) !=\"Mn\")   # unicode category of text which is not equal to Nonspacing Mark (Mn) or non markmarking space  , \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7fjRKvtPS_z",
        "outputId": "9b14b198-d3e0-4425-edc6-9713328ce3ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example for accent normalisation , test function\n",
        "unicodeToAscii('Français , Passionné de données')\n",
        "      # output: Francais , Passionne de donnees"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Francais , Passionne de donnees'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clpBJxkoCfK_"
      },
      "source": [
        "# Lowercase , trim white spaces , lines ....etc ,  and remove non-letter character\n",
        "\n",
        "def normalizeString(s):\n",
        "  \n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "\n",
        "  # replace any \".!?\" by whitespace + the character -->\"|\" = \" |\". \\1 means the first bracketed group  -->[.!?]. \n",
        "  # r is to not consider \\1 as a character ( r to escape a backslash).  \n",
        "\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  # print(s)\n",
        "\n",
        "  # Remove any character that is not a sequence of lower case or upper case character , + means one or more\n",
        "\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "  # print(s)\n",
        "\n",
        "  # Remove a sequence of white space characte \n",
        "\n",
        "  s = re.sub(r\"\\s+\",r\" \" , s).strip()\n",
        "\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxOW8CMdIe7h",
        "outputId": "1b8137af-0477-43c6-9d36-a990a249cdef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example for normalizeString , test function\n",
        "normalizeString(\"aab1234asdf!s's   ss?\")\n",
        "# output:        aab asdf !s s ss ?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aab asdf !s s ss ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fccE0NECe4g",
        "outputId": "8e3529ef-7117-4d90-9f3c-572d3b9f41fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Visualising the file if it is properly written\n",
        "\n",
        "# New_formatted = '/content/drive/My Drive/Datasets_Data_Science/cornell movie-dialogs corpus/New_formatted_Movie_conversations.txt'\n",
        "\n",
        "print(\"Execution Started ....\")\n",
        "# Read the files and split into lines\n",
        "read_lines = open(New_formatted , encoding=\"utf-8\").read().strip().split('\\n')\n",
        "# print(read_lines[:10])\n",
        "\n",
        "# Split every line into pairs and normalize\n",
        "\n",
        "normalize_pairs = [[normalizeString(s) for s in pair.split(\"\\t\")] for pair in read_lines]\n",
        "# print(normalize_pairs[0:10])\n",
        "\n",
        "voc = Vocabulary(\"cornell movie-dialogs corpus\")\n",
        "\n",
        "print(\"Execution Done ....\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Execution Started ....\n",
            "Execution Done ....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXnKIo0fCeeM",
        "outputId": "fb657e6d-08ec-49cc-ecf0-b708852ee6df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"length of the normalized pairs :\" ,len(normalize_pairs) ,\"\\n\")\n",
        "normalize_pairs[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of the normalized pairs : 221282 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
              "  'well i thought we d start with pronunciation if that s okay with you .'],\n",
              " ['well i thought we d start with pronunciation if that s okay with you .',\n",
              "  'not the hacking and gagging and spitting part . please .'],\n",
              " ['not the hacking and gagging and spitting part . please .',\n",
              "  'okay . . . then how bout we try out some french cuisine . saturday ? night ?'],\n",
              " ['you re asking me out . that s so cute . what s your name again ?',\n",
              "  'forget it .'],\n",
              " ['no no it s my fault we didn t have a proper introduction', 'cameron .']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jStJeQIiE7hD",
        "outputId": "08c3b1ae-0ff5-4396-9229-db11c3666dc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This code is for testing\n",
        "\n",
        "print(len(normalize_pairs[0][0].split())<10)\n",
        "len(normalize_pairs[0][1].split())<10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXtzLlNcCd4x"
      },
      "source": [
        "# Return true if both sentences  in the normalize_pairs \"P\" are under the max length thrishold\n",
        "\n",
        "MAX_LENGTH = 10    # maximum sentence lenth to be considered\n",
        "def filterPair(p): # Initial function to filter later\n",
        "# Input sequence needs to be preserve  the last word for EOS taken\n",
        "\n",
        "  return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n",
        "\n",
        "# Filter pair using filter pair condition (reccursion finction)\n",
        "def filterPairs(pairs1):\n",
        "  return [pair for pair in pairs1 if filterPair(pair)]  # filterig the pair by recurssion for each pair in input sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvbfA72BCdDU",
        "outputId": "3838034c-cfda-4875-dcc7-3839cad60de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print( \"There are {} pairs/conversations in the dataset \".format(len(normalize_pairs)))\n",
        "\n",
        "filtered_pairs = filterPairs(normalize_pairs)\n",
        "\n",
        "print(\"After filtering there are {} pairs in the dataset\".format(len(filtered_pairs)))\n",
        "\n",
        "# filtered_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 221282 pairs/conversations in the dataset \n",
            "After filtering there are 64271 pairs in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeCdh_R9PjEK",
        "outputId": "f686a669-f9a7-400c-bd91-c975a8b262b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filtered_pairs[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['there .', 'where ?'],\n",
              " ['you have my word . as a gentleman', 'you re sweet .'],\n",
              " ['hi .', 'looks like things worked out tonight huh ?'],\n",
              " ['you know chastity ?', 'i believe we share an art instructor'],\n",
              " ['have fun tonight ?', 'tons']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLOO9EPkInIa",
        "outputId": "c7e91976-62f5-44c5-d39d-ea689eafe62c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loop through each pair of and add the question and replace sentence to the vocabulary\n",
        "\n",
        "for pair in filtered_pairs:\n",
        "  voc.addSentence(pair[0])\n",
        "  voc.addSentence(pair[1])\n",
        "print(\"counted words : \" , voc.num_words , \"\\n\")\n",
        "\n",
        "print(\"pairs :\" , \"\\n\" )\n",
        "for pair in filtered_pairs[0:5]:\n",
        "  print(pair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "counted words :  18008 \n",
            "\n",
            "pairs : \n",
            "\n",
            "['there .', 'where ?']\n",
            "['you have my word . as a gentleman', 'you re sweet .']\n",
            "['hi .', 'looks like things worked out tonight huh ?']\n",
            "['you know chastity ?', 'i believe we share an art instructor']\n",
            "['have fun tonight ?', 'tons']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOs7VpOUInTT"
      },
      "source": [
        "MIN_COUNT= 3 # minimum wordcount thrishold for trimming\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "  # Trim wordds used under min count for voc\n",
        "  voc.trim(MIN_COUNT)\n",
        "  # Filter out pairs with trimmed words\n",
        "  keep_pairs = []\n",
        "  for pair in pairs:\n",
        "    input_sentence = pair[0]\n",
        "    output_sentence = pair[1]\n",
        "    keep_input =  True\n",
        "    keep_output= True\n",
        "\n",
        "    # Check input sentence\n",
        "    for word in input_sentence.split(\" \"):\n",
        "      if word not in voc.word2index:\n",
        "        keep_input = False\n",
        "        break\n",
        "\n",
        "    # check output sentence\n",
        "\n",
        "    for words in output_sentence.split(\" \"):\n",
        "      if words not in voc.word2index:\n",
        "        keep_output = False\n",
        "        break\n",
        "\n",
        "    # only keep pairs that do not contain  trimmed word(s) in there input or output sentence\n",
        "    if keep_input and keep_output:\n",
        "      keep_pairs.append(pair)\n",
        "\n",
        "  print(\"Trimmed from {} pairs to {} , {:4f} of total\".format(len(pairs), len(keep_pairs) , len(keep_pairs)/len(pairs)))\n",
        "\n",
        "  return keep_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RsprvShRXcy",
        "outputId": "859951e0-bccd-4bbb-ea2c-def62b9c6a8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Trim voc and pairs  # This will use it for further processing\n",
        "pairs_trim = trimRareWords(voc, filtered_pairs, MIN_COUNT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keep_words 7823 / 18005 = 0.4345\n",
            "Trimmed from 64271 pairs to 53165 , 0.827200 of total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fZ733yOqFaq",
        "outputId": "e075a550-a703-4129-f3c5-192f13d446cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pairs_trim[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you have my word . as a gentleman', 'you re sweet .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeXQI_LXT82b"
      },
      "source": [
        "## Prepare Data for Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ToQjvpmoHwv"
      },
      "source": [
        "# Experiment Code\n",
        "def indexesFromSentences(voc , sentence):\n",
        "  return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_TOKEN]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM9DkRpba5zl",
        "outputId": "30d34e39-e2c3-4b6f-e500-c7917bd1c5ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "  # Testing the function\n",
        "print(indexesFromSentences(voc, pairs_trim[1][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7, 8, 9, 10, 4, 11, 12, 13, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcceuNCSa54V",
        "outputId": "110ca33d-6b37-4908-d8df-e5667f6a78f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define some samples for testing converting vocabulary of the input sentence to EOS_tokens\n",
        "\n",
        "input_sent = []\n",
        "output_sent = []\n",
        "\n",
        "for pair in tqdm.tqdm(pairs_trim[0:10]):\n",
        "  input_sent.append(pair[0])\n",
        "  output_sent.append(pair[1])\n",
        "\n",
        "print(input_sent)\n",
        "indexes = [indexesFromSentences(voc, sentence) for sentence in input_sent]\n",
        "indexes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 21575.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['there .', 'you have my word . as a gentleman', 'hi .', 'have fun tonight ?', 'well no . . .', 'then that s all you had to say .', 'but', 'do you listen to this crap ?', 'what good stuff ?', 'wow']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 4, 2],\n",
              " [7, 8, 9, 10, 4, 11, 12, 13, 2],\n",
              " [16, 4, 2],\n",
              " [8, 31, 22, 6, 2],\n",
              " [33, 34, 4, 4, 4, 2],\n",
              " [35, 36, 37, 38, 7, 39, 40, 41, 4, 2],\n",
              " [42, 2],\n",
              " [47, 7, 48, 40, 45, 49, 6, 2],\n",
              " [50, 51, 52, 6, 2],\n",
              " [58, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VncNRouLBnI",
        "outputId": "6863605d-2ef8-480e-ef42-6ed55d81f7bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(itertools.zip_longest(*indexes)) # we require this transpose to sent it to LSTM for attention score processing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 7, 16, 8, 33, 35, 42, 47, 50, 58),\n",
              " (4, 8, 4, 31, 34, 36, 2, 7, 51, 2),\n",
              " (2, 9, 2, 22, 4, 37, None, 48, 52, None),\n",
              " (None, 10, None, 6, 4, 38, None, 40, 6, None),\n",
              " (None, 4, None, 2, 4, 7, None, 45, 2, None),\n",
              " (None, 11, None, None, 2, 39, None, 49, None, None),\n",
              " (None, 12, None, None, None, 40, None, 6, None, None),\n",
              " (None, 13, None, None, None, 41, None, 2, None, None),\n",
              " (None, 2, None, None, None, 4, None, None, None, None),\n",
              " (None, None, None, None, None, 2, None, None, None, None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CL9Wjbea5k4"
      },
      "source": [
        "# Appliying zero padding as ther input size should be retained , this is hust like padding= \"same\" in tensorflow.\n",
        "\n",
        "def zero_padding(l, fill_value = 0):\n",
        "  return list(itertools.zip_longest(*l , fillvalue=fill_value))         # *l will transpose the matrix for ex: [1,2] becomes [1]\n",
        "                                                                                                                           # [2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYGVxOQ2zkn-",
        "outputId": "c2ab8dde-0f34-4a7a-fa9c-5bd9e251aa51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Inde_length = [len(inde) for inde in tqdm.tqdm(indexes)]  # Storing the length of each list in Inde_length\n",
        "max(Inde_length)                                          # getting maximum length of each list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 55043.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tuzk-1EMOog",
        "outputId": "da530be2-88c6-40a3-f3be-66cec7b239b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Inde_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 3, 5, 6, 10, 2, 8, 5, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laElEGp-zkg2",
        "outputId": "68fe4728-3f0d-485c-806a-079ae2f48535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test the function\n",
        "\n",
        "test_results = zero_padding(indexes)\n",
        "print(len(test_results))\n",
        "test_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 7, 16, 8, 33, 35, 42, 47, 50, 58),\n",
              " (4, 8, 4, 31, 34, 36, 2, 7, 51, 2),\n",
              " (2, 9, 2, 22, 4, 37, 0, 48, 52, 0),\n",
              " (0, 10, 0, 6, 4, 38, 0, 40, 6, 0),\n",
              " (0, 4, 0, 2, 4, 7, 0, 45, 2, 0),\n",
              " (0, 11, 0, 0, 2, 39, 0, 49, 0, 0),\n",
              " (0, 12, 0, 0, 0, 40, 0, 6, 0, 0),\n",
              " (0, 13, 0, 0, 0, 41, 0, 2, 0, 0),\n",
              " (0, 2, 0, 0, 0, 4, 0, 0, 0, 0),\n",
              " (0, 0, 0, 0, 0, 2, 0, 0, 0, 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHx8vRxoUuop",
        "outputId": "decdf0cb-0fe1-49d1-fdfb-a9292e77bbab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Experement\n",
        "matrix = []\n",
        "for i , seq in enumerate(tqdm.tqdm(test_results)):               # enumerating the value in the sequence\n",
        "  matrix.append([])\n",
        "\n",
        "matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 42711.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], [], [], [], [], [], [], [], [], []]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7XBSXWizkWy"
      },
      "source": [
        "def binary_matrix(lis , values = 0):\n",
        "  matrix = []\n",
        "  for i , seq in enumerate(tqdm.tqdm(lis)):    # enumerating the value in the sequence\n",
        "    matrix.append([])                          # Creating Structure to store the values\n",
        "    for token in seq:                          # taking values from the token\n",
        "      if token == PAD_TOKEN:                   # checking if token is a number or is = to PAD_TOKEN which is \"0\"\n",
        "        matrix[i].append(0)\n",
        "      else:\n",
        "        matrix[i].append(1)\n",
        "  return matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHKR-yn2Mljy",
        "outputId": "61af76b4-f6dd-4ae8-df9d-e5318eaa1027",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "binary_result = binary_matrix(test_results)               # This is mask for the tensors/matrix for our output matrix\n",
        "binary_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 41692.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 0, 1, 1, 0],\n",
              " [0, 1, 0, 1, 1, 1, 0, 1, 1, 0],\n",
              " [0, 1, 0, 1, 1, 1, 0, 1, 1, 0],\n",
              " [0, 1, 0, 0, 1, 1, 0, 1, 0, 0],\n",
              " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
              " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
              " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2pr2J4JMkXM"
      },
      "source": [
        "# Return padding  input sequence tensors and as well as a tensor of length for each  of the sequence in batch\n",
        "\n",
        "def inputVar(in_var , voc):                                     # Input only Questions in pair \n",
        "  indexes_batch = [indexesFromSentences(voc,sentence) for sentence in in_var]  # Apply Indexsfromsentence function\n",
        "  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])  # Getting the length of all the indexes\n",
        "  pad_List = zero_padding(indexes_batch)                # Applying Zero_padding to indexes batch\n",
        "  padVar  = torch.LongTensor(pad_List)                    # indices (LongTensor) – the indices into self\n",
        "  return padVar , lengths                                                \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkpRMyTwMkUU"
      },
      "source": [
        "# Return padding target sequence tensor , padding mask , and max target lengths\n",
        "\n",
        "def outputVar(Out_var , voc):\n",
        "  indexes_batch = [indexesFromSentences(voc,sentence) for sentence in Out_var]  \n",
        "  max_target_lengths = max([len(inde) for inde in indexes])      # Getting only max length of the lists\n",
        "  padList = zero_padding(indexes_batch)\n",
        "  mask = binary_matrix(padList)\n",
        "  mask = torch.ByteTensor(mask)                                  # to make sure that the binary_matrix/mask is either \"0\" or \"1\"\n",
        "  padVar = torch.LongTensor(padList)\n",
        "  return padVar , mask, max_target_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2TWSaUj1Au"
      },
      "source": [
        "# Return all items for a given batch of pairs\n",
        "\n",
        "def batch2TrainData(voc ,  pair_batch):\n",
        "  # Sort the Questions in deceending length\n",
        "  pair_batch.sort(key=lambda x: len(x[0].split(' ')) ,  reverse=True)  # here we are taking length of input Question to sort \n",
        "\n",
        "  input_batch, output_batch = [] , []                                            # Defining two empty lists for capturing the data for input batch and output batch\n",
        "  for pair in pair_batch:\n",
        "    input_batch.append(pair[0])\n",
        "    output_batch.append(pair[1])\n",
        "\n",
        "  input_ ,  length = inputVar(input_batch , voc)                              # Function taken from InputVar\n",
        "  output, mask, max_target_lengths = outputVar(output_batch , voc)            # Function taken from OutputVar\n",
        "\n",
        "  return input_ , length, output, mask , max_target_lengths                   # returning all the results for batch to process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIraWYn5j09a",
        "outputId": "edd40a39-7f3c-4bbb-892a-b81522dc1afb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For testing we will take a very small batch of (5):\n",
        "\n",
        "small_batch_size = 5\n",
        "\n",
        "batches = batch2TrainData(voc , [random.choice(pairs_trim) for _ in range(small_batch_size)])\n",
        "input_, lengths, output , mask, max_target_len = batches\n",
        "\n",
        "print(\"\\n\",\"input_variable:\", input_)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", output)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 50533.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " input_variable: tensor([[  25,  147,   25,   25, 2074],\n",
            "        [ 200,   92, 1441,  200, 2074],\n",
            "        [ 459,    7,  253, 1469,    4],\n",
            "        [  61,  278,  376,    4,    2],\n",
            "        [  37,    4,    4,    2,    0],\n",
            "        [ 123,    2,    2,    0,    0],\n",
            "        [  40,    0,    0,    0,    0],\n",
            "        [1528,    0,    0,    0,    0],\n",
            "        [   4,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "lengths: tensor([10,  6,  6,  5,  4])\n",
            "target_variable: tensor([[ 122,   88,   42,   33,  318],\n",
            "        [  25, 3431,   67,    7, 1597],\n",
            "        [ 200,    4,  253,  290,    4],\n",
            "        [2378,   92, 1442,    8,    2],\n",
            "        [  40,    7,    4, 1468,    0],\n",
            "        [ 280,  123,    2,   70,    0],\n",
            "        [   4,   96,    0, 2553,    0],\n",
            "        [   2,    3,    0,    4,    0],\n",
            "        [   0,    6,    0,    2,    0],\n",
            "        [   0,    2,    0,    0,    0]])\n",
            "mask: tensor([[1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 0, 1, 0],\n",
            "        [1, 1, 0, 1, 0],\n",
            "        [0, 1, 0, 1, 0],\n",
            "        [0, 1, 0, 0, 0]], dtype=torch.uint8)\n",
            "max_target_len: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMDqK0erIrK0"
      },
      "source": [
        "# Defining Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wPECakmbQcc"
      },
      "source": [
        "## EncoderRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzhp6GeSIy32"
      },
      "source": [
        "class EncoderRNN(nn.Module):                                              # nn.Module base class will be overridden by forward()\n",
        "  def __init__(self, hidden_size , embedding , n_layers=1, dropout=0):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.embedding = embedding\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Initialize GRU , the input_size and hidden_size params are both set to 'hidden_size'\n",
        "    # because out input_size is a word embedding with number of features == 'hidden_size'\n",
        "\n",
        "    self.gru = nn.GRU(hidden_size , hidden_size , n_layers , dropout=(0 if n_layers ==1 else dropout), bidirectional=True)\n",
        "\n",
        "\n",
        "  def forward(self, input_seq , input_length , hidden=None):\n",
        "    # input_seq: batch of input sentence ; shape=(max_length , batch_size)\n",
        "    # input_length: list of sequence lengths corusponding to each sentence in the batch\n",
        "    # hidden_state of shape : (n_layers x num_directions, batch_size , hidden_size)\n",
        "\n",
        "    # Convert word index to embedding\n",
        "    embedded = self.embedding(input_seq)\n",
        "\n",
        "\n",
        "    # pack padded batch of sequences for RNN Module\n",
        "    packed = torch.nn.utils.rnn.pack_padded_sequence(embedded , input_length)\n",
        "\n",
        "    # Forward pass through GRU\n",
        "    outputs , hidden = self.gru(packed, hidden)\n",
        "\n",
        "    # unpack padding\n",
        "    outputs, _ = torch.nn.utils.rnn.pad_padded_sequence(outputs)\n",
        "\n",
        "    # Sum bidirectional GRU outputs\n",
        "    outputs = outputs[:,:,:self.hidden_size] +outputs[:,:,self.hidden_size:]\n",
        "\n",
        "    # return output and final hidden state\n",
        "    return output, hidden\n",
        "\n",
        "    # outputs: the output features h_t  from the last layer of the GRU, for each timestamp (sum of bidirectional outputs)\n",
        "    # outputs shape = (max_length, batch_size , hidden_size)\n",
        "    # hidden : hidden  state for the last timestep , of shape =(n_layers x num_directions , batch_size, hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8sMrDUiIzKj",
        "outputId": "e71b00da-e347-4ed8-80c1-a4c4e7ed3cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Just an example for Gru \n",
        "seed = 100\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "rnn= nn.GRU(6,20,2)                           # GRU(input_size , hidden_size , num_layer)\n",
        "input = torch.randn(5,3,6)                    # (seq, batch , input)\n",
        "h0 = torch.randn(2,3,20)                      # (num_layers , batch, hidden)\n",
        "output, hn = rnn(input,h0)                    \n",
        "print(output.shape)                           # (seq, batch , hidden_size)\n",
        "print(hn.shape)                               # (layer,  batch, hidden_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 20])\n",
            "torch.Size([2, 3, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dOZImTjrUDf",
        "outputId": "817349a0-c400-44b0-a12e-69fde2d69fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4235, -1.6828,  0.0068,  2.0669, -0.0146, -0.4219],\n",
              "         [ 0.9035,  0.5738,  0.3317,  1.0563, -0.1278,  0.4790],\n",
              "         [ 1.6395,  1.5672, -0.2032,  0.3842, -0.7113, -0.2225]],\n",
              "\n",
              "        [[-0.5376,  0.0094, -0.1541,  0.7763,  0.3985, -0.2435],\n",
              "         [ 1.0503,  0.5013, -1.3130,  0.7133,  0.4644,  1.3323],\n",
              "         [ 0.6154, -0.4712, -0.5738,  0.5994, -0.6708,  0.1534]],\n",
              "\n",
              "        [[ 0.2652, -0.3506,  0.3225, -0.5089,  0.2242,  0.6174],\n",
              "         [ 0.4700, -1.1933, -1.7227, -0.5616,  1.6748,  0.1667],\n",
              "         [-0.2521, -0.9129,  1.4716,  0.0738,  0.7192, -0.1969]],\n",
              "\n",
              "        [[ 1.1567, -0.7507,  0.7029, -1.0551, -0.5565, -0.3290],\n",
              "         [ 0.7203,  1.4243, -1.0327,  0.6703,  0.6702,  1.2824],\n",
              "         [ 1.2131, -0.3373, -0.5130,  1.2732, -0.5937,  1.0812]],\n",
              "\n",
              "        [[-0.5178,  1.0765, -0.8165, -1.4601,  1.1709, -1.0173],\n",
              "         [ 0.5260,  1.4662,  0.1346, -0.4679, -0.4754, -0.8853],\n",
              "         [ 0.0866, -0.8976,  1.0008, -0.4557, -1.4694, -2.4440]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWKHjlNPooBW",
        "outputId": "1538d057-5e82-49f3-d537-cd72009220b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "input[:,:,:2]                          # Just an example for Gru "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4235, -1.6828],\n",
              "         [ 0.9035,  0.5738],\n",
              "         [ 1.6395,  1.5672]],\n",
              "\n",
              "        [[-0.5376,  0.0094],\n",
              "         [ 1.0503,  0.5013],\n",
              "         [ 0.6154, -0.4712]],\n",
              "\n",
              "        [[ 0.2652, -0.3506],\n",
              "         [ 0.4700, -1.1933],\n",
              "         [-0.2521, -0.9129]],\n",
              "\n",
              "        [[ 1.1567, -0.7507],\n",
              "         [ 0.7203,  1.4243],\n",
              "         [ 1.2131, -0.3373]],\n",
              "\n",
              "        [[-0.5178,  1.0765],\n",
              "         [ 0.5260,  1.4662],\n",
              "         [ 0.0866, -0.8976]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ-0Nttssud2"
      },
      "source": [
        "## DecoderRNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk4wuKE8tIaO"
      },
      "source": [
        "### Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J--HOK2JtGmy"
      },
      "source": [
        "# Luong attention layer\n",
        "\n",
        "class Attn(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, Method ,  hidden_size):\n",
        "    super(Attn, self).__init__()\n",
        "    self.Method = Method\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "\n",
        "    def dot_score(self, hidden , encoder_output):                 # Example given below\n",
        "      return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "      #hidden of shape : (1,batch_size, hidden_size)\n",
        "      # encoder_outputs of shape :(max_length , batch_size , hidden_size)\n",
        "\n",
        "      # This is what we will get (1,batch_size, hidden_size) * (max_length , batch_size , hidden_size) = (max_length , batch_size , hidden_size)\n",
        "\n",
        "      # calculate the attention weights (energies)\n",
        "\n",
        "      attn_energies = self.dot_score(hidden , encoder_output)   # (max_length , batch_size)\n",
        "\n",
        "      # Transpose max length  and batch_size dimension\n",
        "\n",
        "      attn_energies = attn_energies.t()                 # (batch_size , max_length)\n",
        "\n",
        "      # Return the softmax normalized probability score (unsqeeze means added dimension \"1\" in the 1 position ex:(0,1,2 ))\n",
        "      return F.softmax(attn_energies, dim=1).unsqeeze(1)    #(batch_size,1 ,max_length) dim=1 means Softmax accross the columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJU2npbpK9h7",
        "outputId": "bb8bc6c7-0334-45bf-f172-ab6933e732f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "seed = 100\n",
        "import torch\n",
        "a= torch.randn(5,3,7)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0669,  1.4491,  1.6847,  0.7856,  1.2006, -0.3105, -0.3280],\n",
              "         [ 0.5235,  0.4513, -0.0601, -0.7814, -0.0250,  1.3050,  0.6381],\n",
              "         [ 1.8029, -0.0042,  0.1457, -1.2177, -0.9411,  1.9206,  0.4402]],\n",
              "\n",
              "        [[ 1.4200,  1.2067, -1.0484,  0.3757,  1.0247, -1.0756,  0.9970],\n",
              "         [ 0.8325, -0.7680, -0.3911, -0.3193, -1.2576,  1.2911,  0.6845],\n",
              "         [ 0.6268,  0.6050,  0.9841, -0.4599, -0.9559,  0.8190, -1.7799]],\n",
              "\n",
              "        [[ 0.4236, -1.2397,  0.5207, -0.8777, -0.4465,  0.6655,  0.5522],\n",
              "         [-0.4974, -1.3612, -1.0280, -1.1992,  0.5143, -0.1974, -0.0092],\n",
              "         [-0.2363,  0.1232, -0.0661, -0.6769,  0.0068, -0.0310, -0.5303]],\n",
              "\n",
              "        [[ 0.2819,  0.5358, -0.7164,  1.0563,  0.2161,  0.8604, -0.2853],\n",
              "         [ 0.7365, -1.6303,  0.1452,  0.1118, -0.0449,  0.4897, -1.3087],\n",
              "         [-0.7419, -0.3256, -0.4453, -0.8108,  1.2759, -0.0172, -1.0334]],\n",
              "\n",
              "        [[-0.8411,  0.6721,  0.5990,  0.1652, -2.4144, -1.6311,  1.1722],\n",
              "         [-0.9337,  1.3877, -0.5797,  1.3255, -0.9877, -1.1825,  0.4162],\n",
              "         [ 2.0649,  0.2681, -0.9893,  0.1578,  1.0996,  0.3683,  0.8855]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psz_R22DLf-D",
        "outputId": "b9ebd7a3-c0e2-4a30-81b5-0747e01df52b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "torch.sum(a,dim=2) # Summing all the columns "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.5484,  2.0513,  2.1464],\n",
              "        [ 2.9003,  0.0720, -0.1608],\n",
              "        [-0.4020, -3.7781, -1.4105],\n",
              "        [ 1.9488, -1.5008, -2.0984],\n",
              "        [-2.2780, -0.5542,  3.8549]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4U3FrpAMJPl",
        "outputId": "243c7558-d6d6-42d1-cd3e-324964d2e345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list1 = [ 0.0669,  1.4491,  1.6847,  0.7856,  1.2006, -0.3105, -0.3280]\n",
        "list2 = [ 0.5235,  0.4513, -0.0601, -0.7814, -0.0250,  1.3050,  0.6381]\n",
        "list3 = [ 1.8029, -0.0042,  0.1457, -1.2177, -0.9411,  1.9206,  0.4402]\n",
        "print(sum(list1) , round(sum(list2),6) , sum(list3))            # first line and the this line is simillar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.5484 2.0514 2.1464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyxA3VKiV4a5"
      },
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, attn_model , embedding, hidden_size , output_size , n_layers = 1  dropout=0.1):\n",
        "    super(LuongAttnDecoderRNN , self).__init__()\n",
        "    self.attn_model = attn_model\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout = dropout\n",
        "\n",
        "    # Define layers\n",
        "    self.embedding = embedding\n",
        "    self.embedding_dropout = nn.Dropout(dropout)\n",
        "    self.gru = nn.GRU(hidden_size , hidden_size , n_layers , dropout = (0 if n_layers == 1 else dropout)\n",
        "    self.concat = nn.Linear(hidden_size * 2 , hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    self.attn = Attn(attn_model , hidden_size)\n",
        "\n",
        "  def forward(self, input_step , last_hidden , encoder_outputs ):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbCk83qGs0Gx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B44yu85s0Dx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVBP_m1ui8LA"
      },
      "source": [
        "# **Appendix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtjJbO-kSfVr"
      },
      "source": [
        "\n",
        "\n",
        "# eval() :                                  Evaluate the given source in the context of globals and locals.\n",
        "#                                           The source may be a string representing a Python expression or a code object as returned by compile().\n",
        "#                                           The globals must be a dictionary and locals can be any mapping, defaulting to the current globals and locals.\n",
        "#                                           If only globals is given, locals defaults to it.\n",
        "\n",
        "# *****************************************************************************************************\n",
        "\n",
        "# .strip() :                                Strips binary objects and custom things depending on the argument given\n",
        "\n",
        "# *****************************************************************************************************\n",
        "\n",
        "# csv.writer() : sv_writer =                csv.writer(fileobj [, dialect='excel'] [optional keyword args])\n",
        "#                                           for row in sequence:\n",
        "#                                             csv_writer.writerow(row)\n",
        "#                                           [or]\n",
        "#                                           csv_writer = csv.writer(fileobj [, dialect='excel'] [optional keyword args])\n",
        "#                                            csv_writer.writerows(rows)\n",
        "\n",
        "# *****************************************************************************************************\n",
        "   \n",
        "# codecs                                    str(*args, **kwargs)\n",
        "#                                           str(object='') -> str\n",
        "#                                           str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
        "\n",
        "#                                           Create a new string object from the given object. If encoding or\n",
        "#                                           errors is specified, then the object must expose a data buffer\n",
        "#                                           that will be decoded using the given encoding and error handler.\n",
        "#                                           Otherwise, returns the result of object.str() (if defined)\n",
        "#                                           or repr(object).\n",
        "#                                           encoding defaults to sys.getdefaultencoding().\n",
        "#                                           errors defaults to 'strict'.\n",
        "\n",
        "# *****************************************************************************************************\n",
        "\n",
        "# codecs.decode                             def codecs.decode(obj, encoding='utf-8', errors='strict')\n",
        "#                                           Decodes obj using the codec registered for encoding.\n",
        "\n",
        "#                                           Default encoding is 'utf-8'.  errors may be given to set a\n",
        "#                                           different error handling scheme.  Default is 'strict' meaning that encoding\n",
        "#                                           errors raise a ValueError.  Other possible values are 'ignore', 'replace'\n",
        "#                                           and 'backslashreplace' as well as any other name registered with\n",
        "#                                           codecs.register_error that can handle ValueErrors.\n",
        "\n",
        "# *****************************************************************************************************\n",
        "\n",
        "# tqdm.tqdm()                                def tqdm.tqdm(iterable=None, desc=None, total=None, leave=True, file=None, ncols=None, mininterval=0.1, \\\n",
        "#                                            maxinterval=10.0, miniters=None, ascii=None, disable=False, unit='it', unit_scale=False, dynamic_ncols=False, \\\n",
        "#                                            smoothing=0.3, bar_format=None, initial=0, position=None, postfix=None, unit_divisor=1000, write_bytes=None, \\\n",
        "#                                            lock_args=None, gui=False, **kwargs)\n",
        "\n",
        "#                                            Decorate an iterable object, returning an iterator which acts exactly\n",
        "#                                            like the original iterable, but prints a dynamically updating\n",
        "#                                            progressbar every time a value is requested.\n",
        "\n",
        "# *****************************************************************************************************\n",
        "\n",
        "# NFD:                                        Normalization Form Canonical Decomposition\tCharacters are decomposed by canonical equivalence, \n",
        "#                                             and multiple combining characters are arranged in a specific order.\n",
        "\n",
        "# Mn :                                        Nonspacing Mark (Mn)\n",
        "\n",
        "# *****************************************************************************************************\n",
        "\n",
        "# re.sub                                      re.sub(pattern, repl, string, count=0, flags=0)\n",
        "#                                             Return the string obtained by replacing the leftmost\n",
        "#                                             non-overlapping occurrences of the pattern in string by the\n",
        "#                                             replacement repl.  repl can be either a string or a callable;\n",
        "#                                             if a string, backslash escapes in it are processed.  If it is\n",
        "#                                             a callable, it's passed the match object and must return\n",
        "#                                             a replacement string to be used.\n",
        "\n",
        "# *****************************************************************************************************\n",
        "\n",
        "#nn.module                                    Base class for all neural network modules.\n",
        "#                                             Your models should also subclass this class.\n",
        "#                                             Modules can also contain other Modules, allowing to nest them in\n",
        "#                                             a tree structure. You can assign the submodules as regular attributes:\n",
        "\n",
        "#                                             import torch.nn as nn\n",
        "#                                             import torch.nn.functional as F\n",
        "\n",
        "#\t\t\t\t\t\t\t                                class Model(nn.Module):\n",
        "#\t\t\t\t\t\t\t                                  def __init__(self):\n",
        "#\t\t\t\t\t\t\t                                    super(Model, self).__init__()\n",
        "# \t\t\t\t\t\t\t                                  self.conv1 = nn.Conv2d(1, 20, 5)\n",
        "# \t\t\t\t\t\t\t                                  self.conv2 = nn.Conv2d(20, 20, 5)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "# \t\t\t\t\t\t\t\t                             def forward(self, x):\n",
        "# \t\t\t\t\t\t\t\t                                x = F.relu(self.conv1(x))\n",
        "# \t\t\t\t\t\t\t\t                                return F.relu(self.conv2(x))\n",
        "# \t\t\t\t\t\t\t\t                            Submodules assigned in this way will be registered, and will have their\n",
        "# \t\t\t\t\t\t\t\t                            parameters converted too when you call to, etc.\n",
        "\n",
        "\n",
        "# *****************************************************************************************************\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUGFglDVYpRZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gpuWLwtYqPr"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I72Q2K_fYux5"
      },
      "source": [
        "# The code is adopted from https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chatbot\n",
        "# and Tutorials from Udemy "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}